{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiyNyIrbRijP"
      },
      "source": [
        "*THIS VERSION INCLUDES*\n",
        "***AUTOMATION***\n",
        "## **Problem Statement**\n",
        "\n",
        "- As most of us are doing our jobs or attending school/college virtually, we often have to attend online meetings and we can’t expect each of our places to always be quiet.\n",
        "\n",
        "- Some of us may live in a noisy environment where we can hear horn sounds or other people’s voices or even sometimes our earphones are at fault which is certainly undesirable for the receiver at the other end.\n",
        "\n",
        "- Being a **Machine learning Engineer**, How can you use neural networks to suppress the background noise, while enhancing the quality and intelligibility of speech.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1G_ZxHchgjq6Q1PWDIlfHlXqba8Q5JzoG\" width=600></center>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAdzKd4pYWJP"
      },
      "source": [
        "## Applications\n",
        "\n",
        "Audio denoising aims at attenuating the noise while retaining the underlying signals. It has various applications like:\n",
        "\n",
        "- Background noise reduction in audio/video calls\n",
        "- Hearing Aids\n",
        "- Automatic Speech Recognition\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1ALn7A6P_H2wuV5SAkbB-JuC62HaNSxP1\" width=300 height=200><img src=\"https://drive.google.com/uc?id=1k8oXZQFfnT-Kam73ZAvm_4Ichqbh0Ku4\" width=300 height=200>\n",
        "<img src=\"https://drive.google.com/uc?id=1mJUZmYmJOs0Y7bSOkEQ1CgiQADhW4zXr\" width=300 height=200>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcY4NVQNdpVG"
      },
      "source": [
        "## What is a Audio Signal?\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1oIMQcHPHX2dfkaPzekJGCIt2c6CkDuzE\" width=500></center>\n",
        "\n",
        "- Sound signals often repeat at regular intervals so that each wave has the same shape.\n",
        "- The height shows the intensity of the sound and is known as the **amplitude**.\n",
        "- The time taken for the signal to complete one full wave is the **period**.\n",
        "- The number of waves made by the signal in one second is called the **frequency**.\n",
        "- The frequency is the reciprocal of the period. The unit of frequency is Hertz.\n",
        "\n",
        "- The majority of sounds we encounter may not follow such simple and regular periodic patterns.\n",
        "- But signals of different frequencies can be added together to create composite signals with more complex repeating patterns.\n",
        "- All sounds that we hear, including our own human voice, consist of waveforms like these. For instance, this could be the sound of a musical instrument.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1lmDK_t8pUJuVigOhavmJdz6Ja18Ul2Fl\" width=400></center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Types of Audio Signals\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1kIeLWSByrX4LlvUGNV0YFmGgLcog1tGQ\" width=500></center>\n",
        "\n",
        "- **Analog**\n",
        " - Continuous values for time (x-axis)\n",
        " - Continuous values for amplitude (y-axis)\n",
        "- **Digital**\n",
        " - Sequence of discrete values\n",
        " - Data points can only take on finite number of values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RkxS15-q72j"
      },
      "source": [
        "_________________________________________________________________________\n",
        "### What is Audio Denoising?\n",
        "\n",
        "> Audio Denoising is the process of removing noises from a speech without affecting the quality of the speech\n",
        "\n",
        "- Here, the noises are any unwanted audio segments for the human hearing like vehicle horn sounds, wind noise, or even static noise.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1fpQUCtp6IrmB1preoUEly_Zx8vj6XDjO\" width=600 height=200></center>\n",
        "\n",
        "- Speech denoising is a long-standing problem.\n",
        "- Given a noisy input signal, the aim is to filter out such noise without degrading the signal of interest.\n",
        "- You can imagine someone talking in a video conference while a piece of music is playing in the background.\n",
        "- In this situation, a speech denoising system has the job of removing the background noise in order to improve the speech signal.\n",
        "- Besides many other use cases, this application is especially important for video and audio conferences, where noise can significantly decrease speech intelligibility.\n",
        "\n",
        "- It is also known as speech enhancement as it enhances the quality of speech.\n",
        "- **Speech enhancement** is an important task and it is used as a preprocessing step in various applications such as audio/video calls, hearing aids, Automatic Speech Recognition (ASR), and speaker recognition.\n",
        "\n",
        "______________________________________________________________________________\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX74qmHuqoWj"
      },
      "source": [
        "**TASK:**\n",
        "- Given a noisy input signal, we aim to build a statistical model that can extract the clean signal and return it to the user.\n",
        "- Here, we focus on source separation of regular speech signals from different types of noise often found in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x8E4lM-tdyT"
      },
      "source": [
        "## Business Constraints\n",
        "\n",
        "1. **Low Latency :** Since our project will run in real-time, we need the whole pipeline to be as much efficient and time-saving as possible.\n",
        "\n",
        "2. **Quality** : As we are dealing with audio, any small error can be caught in the result. Hence, we need the accuracy of our model to be very high.\n",
        "\n",
        "3. **Duration** : The user input audio file should be maximum of 10 minutes duration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4YiMD6ArNyC"
      },
      "source": [
        "## **Methodology :**\n",
        "\n",
        "* ### Before Runtime\n",
        "\n",
        "  * Download the dataset, Clean voice from LibriSpeech and Envionmental Noises from ESC-50 dataset.\n",
        "\n",
        "  * Mix the noisy and clean audio to give our model as input, and we will give the noisy audio as output.\n",
        "\n",
        "  * Convert all the preprocessed audio files in STFT Spectograms for both input and output.\n",
        "  \n",
        "  * Train our model.\n",
        "\n",
        "* ### At runtime\n",
        "  * Check if the audio file is less than 10 minutes\n",
        "\n",
        "  * Takes an Noisy audio file as input.\n",
        "\n",
        " * Model Returns the noisy audio as output.\n",
        "\n",
        " * Remove noise from the input and return it to the user.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbh1-BtV4KO"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?id=1v8CVyXsWIWd6m7Eh-NkmDThLN_lYnXHH\"height=600 width=500></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZCAG0I3vt7j"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugvajNHNtXP7"
      },
      "source": [
        "How can we read and process the audio data?\n",
        "\n",
        "**librosa** is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems.[Click to Know More](https://librosa.org/doc/latest/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zDK8CLo0u5vD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile\n",
        "import IPython\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaF6FlrUwXYj"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7eC9OhvwZ4j"
      },
      "source": [
        "For the process of Audio Denoising, we will use the datasets **LibriSpeech** and **ESC-50**\n",
        "\n",
        "- The clean voices were mainly gathered from LibriSpeech: an ASR corpus based on public domain audiobooks.\n",
        "\n",
        "- The environmental noises were gathered from ESC-50 dataset\n",
        "\n",
        "> **LibriSpeech** is a corpus of approximately 1000 hours of 16kHz read English speech.\n",
        "- The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.[Click to know more](http://www.openslr.org/12/)\n",
        "\n",
        "- In this project, we will use the development set of clean speech dev-clean **dev-clean.tar.gz** for the training purpose.\n",
        "\n",
        "> The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings. [Click to know more](https://github.com/karolpiczak/ESC-50)\n",
        "\n",
        "- The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1fB9vnz8LwzezOfBoihKnePiYqfjeOnWP\"></center>\n",
        "\n",
        "\n",
        "- As you might be imagining at this point, we’re going to use the **ESC-50** dataset as noise signals to the speech examples.\n",
        "- In other words, we first take a small speech signal — this can be someone speaking a random sentence from the **LibriSpeech** dataset.\n",
        "- Then, we add noise to it — such as a woman speaking and a dog barking on the background.\n",
        "- Finally, we use this artificially noisy signal as the input to our CNN model.\n",
        "- The CNN model, in turn, receives this noisy signal and tries to output a clean representation of it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEBzloX6HBVk"
      },
      "source": [
        "We have 2703 clean speech audios and 2000 noisy audios\n",
        "1. The total length of clean audio files is approximately 323 minutes.\n",
        "2. The total length of noisy audio files is approximately 166 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ-nKy3l6Lwc"
      },
      "source": [
        "Now we have downloaded the datasets.\n",
        "\n",
        "Lets start preparing the dataset\n",
        "\n",
        "_______________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNh8ZokJOE41"
      },
      "source": [
        "## How can we train our CNN model with Audio data?\n",
        "\n",
        "Can we convert the audio into a image?\n",
        "\n",
        "The answer is, yes!!. This is done by generating spectograms from the audio.\n",
        "\n",
        "Let's understand what a spectrum is, and use that to understand Spectograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOD4DEsEQbcN"
      },
      "source": [
        "## What is a Spectrum?\n",
        "\n",
        "- As we discussed earlier, signals of different frequencies can be added together to create composite signals, representing any sound that occurs in the real-world.\n",
        "- This means that any signal consists of many distinct frequencies and can be expressed as the sum of those frequencies.\n",
        "\n",
        "- The Spectrum is the set of frequencies that are combined together to produce a signal. eg. the picture shows the spectrum of a piece of music.\n",
        "\n",
        "- The Spectrum plots all of the frequencies that are present in the signal along with the strength or amplitude of each frequency.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1cuiuDuKTKWNgcxTkNthIBLIzeFOMRcjB\"></center>\n",
        "\n",
        "\n",
        "## Time Domain vs Frequency Domain\n",
        "\n",
        "- The waveforms that we saw earlier showing Amplitude against Time are one way to represent a sound signal.\n",
        "- Since the x-axis shows the range of time values of the signal, we are viewing the signal in the Time Domain.\n",
        "\n",
        "- The Spectrum is an alternate way to represent the same signal.\n",
        "- It shows Amplitude against Frequency, and since the x-axis shows the range of frequency values of the signal, at a moment in time, we are viewing the signal in the Frequency Domain.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1M8f0zeKJhM4D_cbnOZq40lVHOyB-0wmH\"></center>\n",
        "\n",
        "\n",
        "## What is a Spectogram?\n",
        "\n",
        "- Since a signal produces different sounds as it varies over time, its constituent frequencies also vary with time.\n",
        "\n",
        "> A **Spectrogram** of a signal plots its Spectrum over time and is like a ‘photograph’ of the signal.It plots Time on the x-axis and Frequency on the y-axis. It is as though we took the Spectrum again and again at different instances in time, and then joined them all together into a single plot.\n",
        "\n",
        "- It uses different colors to indicate the Amplitude or strength of each frequency.\n",
        "- The brighter the color the higher the energy of the signal.\n",
        "- Each vertical ‘slice’ of the Spectrogram is essentially the Spectrum of the signal at that instant in time and shows how the signal strength is distributed in every frequency found in the signal at that instant.\n",
        "\n",
        "- In the example below, the first picture displays the signal in the Time domain ie. Amplitude vs Time.\n",
        "- It gives us a sense of how loud or quiet a clip is at any point in time, but it gives us very little information about which frequencies are present.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=19vaXFsMbnXztRVaEuaK532sStZq_bzds\"></center>\n",
        "\n",
        "- The second picture is the Spectrogram and displays the signal in the Frequency domain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF2Fd7m6Plwt"
      },
      "source": [
        "## How can we generate the Spectograms?\n",
        "\n",
        "> Spectrograms are produced using Fourier Transforms to decompose any signal into its constituent frequencies.\n",
        "\n",
        "- Among time-frequency decompositions, Spectrograms have been proved to be a useful representation for audio processing.\n",
        "- They consist of 2D images representing sequences of **Short Time Fourier Transform** (STFT) with time and frequency as axes, and brightness representing the strength of a frequency component at each time frame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-GGjHQ1Z5kG"
      },
      "source": [
        "Before undertanding Short Time Fourier Transform, Let's Understand what is a Fourier Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRoDZafvZ6Q-"
      },
      "source": [
        "## What is a Fourier Transform of a Audio signal?\n",
        "\n",
        "- An audio signal is a complex signal composed of multiple 'single-frequency sound waves' which travel together as a disturbance(pressure-change) in the medium.\n",
        "- When sound is recorded we only capture the resultant amplitudes of those multiple waves.\n",
        "- Fourier Transform is a mathematical concept that can decompose a signal into its constituent frequencies.\n",
        "- Fourier transform does not just give the frequencies present in the signal, It also gives the magnitude of each frequency present in the signal.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1trJ7ca2q9LBAgTCx3FfEPwYblzGL2-Qy\" width=500></center>\n",
        "\n",
        "### Is extracting the audio features using Fourier Transform efficient?\n",
        "- Audio signals are, in their majority, non-stationary. In other words, the signal’s mean and variance are not constant over time.\n",
        "- Thus, there is not much sense in computing a Fourier Transform over the entire audio signal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRvoy7g4c9BO"
      },
      "source": [
        "Is there any other efficient Method for extracting audio features?\n",
        "\n",
        "- We can use **Short time Fourier Transform** for feature extraction.\n",
        "\n",
        "What is Short Time Fourier Transform?\n",
        "\n",
        "- STFT is application of **Discrete fourier transform** (DFT) over different portions of the audio signal\n",
        "\n",
        "\n",
        "Note: You can read more about STFT [here](https://drive.google.com/file/d/161T88OhfHkV_OamWBCHweBQpTeCCJsQX/view?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbmIaguNcUQy"
      },
      "source": [
        "###Before extracting STFT features from Audio, Let's understand the parameters required for preprocessing of Audio files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chYEdwOhjGjB"
      },
      "source": [
        "**Sample Rate**:\n",
        "- The sample rate is a measurement of the samples per second taken by the system from a audio signal; these frequencies are measured in kilohertz (kHz).\n",
        "- Higher Sample Rate means higher the quality of the audio\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1DRmU_t5V17X5gNSRKkvuXv5h8IGmJiQG\" width=500></center>\n",
        "\n",
        "- Our dataset are recorded at sample rate of 16khz, So we can use sample rate of 8Khz to read the audio, Since 8Khz sample rate is sufficient to sample the audio at acceptable quality.\n",
        "\n",
        "**Minimum Duration**\n",
        "- For Training our model, we will make sure that the duration of audio is atleast 1 sec.\n",
        "\n",
        "**Frame length**\n",
        "- For training the CNN Model, we need the inputs in fixed length.\n",
        "- So we split the audio to several frames of frame length and without overlapping.\n",
        "- We will take frame length as 8064 and frame hope length of 8064.\n",
        "\n",
        "> This is an example of audio divided into 4 frames with non-overlapping windows\n",
        "<center><img src=\"https://drive.google.com/uc?id=1l0usadXRKbruOW6T5xMf4KgkJ178evU5\" width=700></center>\n",
        "\n",
        "**Frame Hop length**\n",
        "- Frame Hop length is the number of samples in between the successive frames.\n",
        "- It should be less than the frame length.\n",
        "- More overlap will give more analysis points and therefore smoother results across time, but the computational expense is proportionately greater.\n",
        "- So we will take Frame hop length as 8064 and frame length as 8064, to avoid overlapping\n",
        "\n",
        "**nb_samples**\n",
        "- It is nothing but the size of input matrix\n",
        "- For example, the input shape (50x8064) shows 50 audio samples, each of frame length 8064.\n",
        "- Higher number of samples may increase the accuracy.\n",
        "- We will train our CNN model with various number of samples to analyze the performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXLMVJ0oc2JS"
      },
      "source": [
        "For training the CNN Model, we need the inputs in fixed length\n",
        "\n",
        "So we will create a function to split the audio to several windows with fixed length(frame_length) and with non-overlapping frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkp2Iy4P0Zht"
      },
      "source": [
        "### Process Diagram of creating data for training\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1ZX4puxx771D762LBFqhmmcm-AO4qOYbh\" ></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ReTajSWLc9aT"
      },
      "outputs": [],
      "source": [
        "def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):\n",
        "    \"\"\"This function take an audio and split into several frame\n",
        "       in a numpy matrix of size (nb_frame,frame_length)\"\"\"\n",
        "\n",
        "    sequence_sample_length = sound_data.shape[0]\n",
        "    # Creating several audio frames using sliding windows\n",
        "    sound_data_list = [sound_data[start:start + frame_length] for start in range(\n",
        "    0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n",
        "    # Combining all the frames to single matrix\n",
        "    sound_data_array = np.vstack(sound_data_list)\n",
        "    return sound_data_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88iE7oDo1TNw"
      },
      "source": [
        "Let's Take a random clean speech and noisy audio  and split the audios into several frames as numpy matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wvI7udCs23sJ"
      },
      "outputs": [],
      "source": [
        "# Required variables for Audio\n",
        "noise_dir=\"/content/noise/\"\n",
        "voice_dir=\"/content/clean_speech/\"\n",
        "path_save_spectrogram=\"/content/spectogram/\"\n",
        "sample_rate=8000\n",
        "min_duration=1.0\n",
        "frame_length=8064\n",
        "hop_length_frame=8064\n",
        "hop_length_frame_noise=5000\n",
        "nb_samples=500\n",
        "n_fft=255\n",
        "hop_length_fft=63\n",
        "dim_square_spec = int(n_fft / 2) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mtV4EpviaHU"
      },
      "source": [
        "We already know that, we will use magnitude spectrograms as a representation of sound in order to predict the noise model to be subtracted to a noisy voice spectrogram.\n",
        "\n",
        "Let's Create a function to blend a random noise to clean speech audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0driw4m-iNI0"
      },
      "outputs": [],
      "source": [
        "def blend_noise_randomly(voice, noise, nb_samples, frame_length):\n",
        "    \"\"\"This function takes as input numpy arrays representing frames\n",
        "    of voice sounds, noise sounds and the number of frames to be created\n",
        "    and return numpy arrays with voice randomly blend with noise\"\"\"\n",
        "\n",
        "    prod_voice = np.zeros((nb_samples, frame_length))\n",
        "    prod_noise = np.zeros((nb_samples, frame_length))\n",
        "    prod_noisy_voice = np.zeros((nb_samples, frame_length))\n",
        "\n",
        "    for i in range(nb_samples):\n",
        "        id_voice = np.random.randint(0, voice.shape[0])\n",
        "        id_noise = np.random.randint(0, noise.shape[0])\n",
        "        level_noise = np.random.uniform(0.2, 0.8)\n",
        "        prod_voice[i, :] = voice[id_voice, :]\n",
        "        prod_noise[i, :] = level_noise * noise[id_noise, :]\n",
        "        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]\n",
        "\n",
        "    return prod_voice, prod_noise, prod_noisy_voice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN4JtBAG7zmg"
      },
      "source": [
        "Now we will try to blend the random noise with the random clean speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV7ooZ5_8Fyp"
      },
      "source": [
        "We have successfully blended the noise with the clean speech. Now lets visualize few samples of clean speech, the noisy speech as time series plot.\n",
        "\n",
        "We have 10 audio samples, each of 8064 frame length. So lets combine all the samples into a single sample for visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFEwsLDjt-l"
      },
      "source": [
        "Now we will extract STFT features from the audio\n",
        "\n",
        "Let's Create a function to convert the audio to magnitude and phase spectograms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUMXmHMkkfDk"
      },
      "outputs": [],
      "source": [
        "def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):\n",
        "    \"\"\"This function takes an audio and convert into spectrogram,\n",
        "       it returns the magnitude in dB and the phase\"\"\"\n",
        "\n",
        "\n",
        "    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n",
        "    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
        "\n",
        "    stftaudio_magnitude_db = librosa.amplitude_to_db(\n",
        "        stftaudio_magnitude, ref=np.max)\n",
        "\n",
        "    return stftaudio_magnitude_db, stftaudio_phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9pwuy5kk8lF"
      },
      "source": [
        "Now let's convert all the audio to magnitude and phase spectograms and save it as matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IE5XOVzYjAxc"
      },
      "outputs": [],
      "source": [
        "def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):\n",
        "    \"\"\"This function takes as input a numpy audi of size (nb_frame,frame_length), and return\n",
        "    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size\n",
        "    (nb_frame,dim_square_spec,dim_square_spec)\"\"\"\n",
        "\n",
        "    # we extract the magnitude vectors from the 256-point STFT vectors and\n",
        "    # take the first 129-point by removing the symmetric half.\n",
        "\n",
        "    nb_audio = numpy_audio.shape[0]\n",
        "    # dim_square_spec = 256/2\n",
        "    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))\n",
        "    m_phase = np.zeros((nb_audio, dim_square_spec, dim_square_spec), dtype=complex)\n",
        "\n",
        "    for i in range(nb_audio):\n",
        "        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(\n",
        "            n_fft, hop_length_fft, numpy_audio[i])\n",
        "\n",
        "    return m_mag_db, m_phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7D2OIAel7Qt"
      },
      "source": [
        "Now we have created the functions required for extracting the magnitude and phase spectogram of the audio files\n",
        "\n",
        "###Let's create a function to combine all the pre-processing to prepare the audio data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4M4rMVger5r"
      },
      "source": [
        "Function to convert audio to frames matrix and combining all the frames matrix to single audio matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kIJ3Z0h2ct1L"
      },
      "outputs": [],
      "source": [
        "def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):\n",
        "    \"\"\"This function take audio files of a directory and merge them\n",
        "    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame\"\"\"\n",
        "\n",
        "    list_sound_array = []\n",
        "\n",
        "    count = 0\n",
        "    for file in list_audio_files:\n",
        "    # open the audio file\n",
        "      try:\n",
        "        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)\n",
        "        # Getting duration of audio file\n",
        "        total_duration = librosa.get_duration(y=y, sr=sr)\n",
        "      except ZeroDivisionError:\n",
        "        count += 1\n",
        "\n",
        "        # Check if the duration is atleast the minimum duration\n",
        "      if (total_duration >= min_duration):\n",
        "          list_sound_array.append(audio_to_audio_frame_stack(\n",
        "              y, frame_length, hop_length_frame))\n",
        "      else:\n",
        "          print(\n",
        "              f\"The following file {os.path.join(audio_dir,file)} is below the min duration\")\n",
        "\n",
        "    return np.vstack(list_sound_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t9PaV74Nltqc"
      },
      "outputs": [],
      "source": [
        "#Data Prepare\n",
        "def create_data(noise_dir, voice_dir,path_save_spectrogram, sample_rate,\n",
        "min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft):\n",
        "    \"\"\"This function will randomly blend some clean voices from voice_dir with some noises from noise_dir\n",
        "    and save the spectrograms of noisy voice, noise and clean voices to disk as well as complex phase,\n",
        "    time series and sounds. This aims at preparing datasets for denoising training. It takes as inputs\n",
        "    parameters defined in args module\"\"\"\n",
        "\n",
        "    list_noise_files = os.listdir(noise_dir)\n",
        "    list_voice_files = os.listdir(voice_dir)\n",
        "\n",
        "    def remove_ds_store(lst):\n",
        "        \"\"\"remove mac specific file if present\"\"\"\n",
        "        if '.DS_Store' in lst:\n",
        "            lst.remove('.DS_Store')\n",
        "\n",
        "        return lst\n",
        "\n",
        "    list_noise_files = remove_ds_store(list_noise_files)\n",
        "    list_voice_files = remove_ds_store(list_voice_files)\n",
        "\n",
        "    nb_voice_files = len(list_voice_files)\n",
        "    nb_noise_files = len(list_noise_files)\n",
        "\n",
        "\n",
        "    # Extracting noise and voice from folder and convert to numpy\n",
        "    noise = audio_files_to_numpy(noise_dir, list_noise_files, sample_rate,\n",
        "                                     frame_length, hop_length_frame_noise, min_duration)\n",
        "\n",
        "    voice = audio_files_to_numpy(voice_dir, list_voice_files,\n",
        "                                     sample_rate, frame_length, hop_length_frame, min_duration)\n",
        "\n",
        "    # Blend some clean voices with random selected noises (and a random level of noise)\n",
        "    prod_voice, prod_noise, prod_noisy_voice = blend_noise_randomly(\n",
        "            voice, noise, nb_samples, frame_length)\n",
        "\n",
        "\n",
        "    # Squared spectrogram dimensions\n",
        "    dim_square_spec = int(n_fft / 2) + 1\n",
        "\n",
        "    # Create Amplitude and phase of the sounds\n",
        "    m_amp_db_voice,  m_pha_voice = numpy_audio_to_matrix_spectrogram(\n",
        "            prod_voice, dim_square_spec, n_fft, hop_length_fft)\n",
        "    m_amp_db_noise,  m_pha_noise = numpy_audio_to_matrix_spectrogram(\n",
        "            prod_noise, dim_square_spec, n_fft, hop_length_fft)\n",
        "    m_amp_db_noisy_voice,  m_pha_noisy_voice = numpy_audio_to_matrix_spectrogram(\n",
        "            prod_noisy_voice, dim_square_spec, n_fft, hop_length_fft)\n",
        "\n",
        "    np.save(path_save_spectrogram + 'voice_amp_db', m_amp_db_voice)\n",
        "    np.save(path_save_spectrogram + 'noise_amp_db', m_amp_db_noise)             #Not required\n",
        "    np.save(path_save_spectrogram + 'noisy_voice_amp_db', m_amp_db_noisy_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m_YTIhDomrIU"
      },
      "outputs": [],
      "source": [
        "noise_dir=\"/content/noise/\"\n",
        "voice_dir=\"/content/clean_speech/\"\n",
        "path_save_spectrogram=\"/content/spectogram/\"\n",
        "sample_rate=8000\n",
        "min_duration=1.0\n",
        "frame_length=8064\n",
        "hop_length_frame=8064\n",
        "hop_length_frame_noise=5000\n",
        "nb_samples=500\n",
        "n_fft=255\n",
        "hop_length_fft=63"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading noisy voice and voice spectograms"
      ],
      "metadata": {
        "id": "sAz5vq6LLTKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-FgDVXecLerz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i2WvL0oPXWk"
      },
      "source": [
        "Here is what the create_data with 500 nb_samples function saves in the directory:\n",
        "<center><img src=\"https://drive.google.com/uc?id=1w79TOMIsAbT1JRuSWBZTuyFAmySiTrTe\" width=600 height=250></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy0wsWOdioQf"
      },
      "source": [
        "___________________________________________________________________\n",
        "## Training model\n",
        "\n",
        "\n",
        "###Two different approaches:\n",
        "\n",
        "1] We can give our model clean_noisy_mix data as input and clean data as output.\n",
        "\n",
        "2] We can give our model clean_noisy_mix data as input and noise data as output, later we subtract noise from the clean_noisy_mix data.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1aKWQcWJJo7XaVRdkcAGATJMz0OHLidEW\" width=500 height=300></center>\n",
        "\n",
        "\n",
        "\n",
        "Methot 1] Giving the model clean data as the output:\n",
        "\n",
        "\n",
        "*  Pros:\n",
        "  *  This approach is simpler, as the model only needs to output the clean audio signal.\n",
        "  *  The model will learn to output the clean signal directly, which may be more efficient and produce better results than trying to estimate the noise and subtract it.\n",
        "*  Cons:\n",
        "   *  The model will not be able to learn about the characteristics of the noise, which may be useful for other tasks (such as identifying and removing specific types of noise).\n",
        "   *   If the noise is very different from one training example to the next, the model may struggle to learn a generalizable denoising function.\n",
        "\n",
        "\n",
        "Method 2] Giving the model noise data as the output:\n",
        "*  Pros:\n",
        "   *   This approach allows the model to learn about the characteristics of the noise, which may be useful for other tasks (such as identifying and removing specific types of noise).\n",
        "   *   If the noise is very different from one training example to the next, the model may be able to learn a more generalizable denoising function by estimating the noise separately from the clean signal.\n",
        "*   Cons:\n",
        "   *   This approach is more complex, as the model needs to output both the noise and the clean signal.\n",
        "   *    The model may struggle to accurately estimate the noise, especially if it is very different from the clean signal.\n",
        "   *   Subtracting the estimated noise from the input signal may not always produce the best results, as the model may not be able to perfectly estimate the noise.\n",
        "\n",
        "Note: Ultimately, which approach is better will depend on your specific goals and the characteristics of your data. You may want to try both approaches and see which one works better for your particular use case.\n",
        "\n",
        "\n",
        "\n",
        "**Here,**\n",
        "In our case, we will be using method 2, as we have a variety of noises available it will be easy for our model to generalize the noise in the audio and will give a better prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzT2UrxR84JO"
      },
      "source": [
        "### Process Diagram of creating data for training\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1ywS_F2KH8taxKhZMgkpOYPEbjdGYip5B\" width=800 height = 400 ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4oG9a9dwgL5"
      },
      "source": [
        "____________________________________________________________________\n",
        "### Model 1: U-net\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1ZXIZzkzQTLHDVnIGEsh5kfus2KVv4lXI\"></center>\n",
        "\n",
        "\n",
        "\n",
        "*   For the preferred configuration the encoder is made of 10 convolutional layers (with LeakyReLU, maxpooling and dropout).\n",
        "*  The decoder is a symmetric expanding path with skip connections.\n",
        "*  The last activation layer is a hyperbolic tangent (tanh) to have an output distribution between -1 and 1.\n",
        "*   Model is compiled with Adam optimizer.\n",
        "*  As our model is performing regression, we used The Mean Squared Error (MSE) the simplest and most common loss function.\n",
        "*   To calculate the MSE, you take the difference between your model’s predictions and the ground truth, square it, and average it out across the whole dataset.\n",
        "*  Read more about U-net [here](https://arxiv.org/abs/1505.04597) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98SFJj0t59u"
      },
      "source": [
        "\n",
        "For our model we used encoder-decoder like model.\n",
        "\n",
        "1. The first model used for the training is a U-Net, a Deep Convolutional Autoencoder with symmetric skip connections. U-Net was initially developed for Biomedical Image Segmentation. Here the U-Net has been adapted to denoise spectrograms.\n",
        "\n",
        "2. In the second model, we will create a Segmentaion Model using Pre-trained model as Encoder.\n",
        "\n",
        "**Note**:  \n",
        "\n",
        "1.   You can learn more about U-net in L9-Object Segmentation in Computer Vision module.\n",
        "2.  Lot of different model were experimented during this project, we will show the results of all the model in the end.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BUn4r-Q5wfnj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "# print(tf.__version__)\n",
        "\n",
        "#Unet network\n",
        "def unet(input_size = (128,128,1)):\n",
        "    #size filter input\n",
        "    size_filter_in = 16\n",
        "    #normal initialization of weights\n",
        "    kernel_init = 'he_normal'\n",
        "    #To apply leaky relu after the conv layer\n",
        "    activation_layer = None\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(inputs)\n",
        "    conv1 = LeakyReLU()(conv1)\n",
        "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv1)\n",
        "    conv1 = LeakyReLU()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool1)\n",
        "    conv2 = LeakyReLU()(conv2)\n",
        "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv2)\n",
        "    conv2 = LeakyReLU()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool2)\n",
        "    conv3 = LeakyReLU()(conv3)\n",
        "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv3)\n",
        "    conv3 = LeakyReLU()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool3)\n",
        "    conv4 = LeakyReLU()(conv4)\n",
        "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv4)\n",
        "    conv4 = LeakyReLU()(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool4)\n",
        "    conv5 = LeakyReLU()(conv5)\n",
        "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv5)\n",
        "    conv5 = LeakyReLU()(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(drop5))\n",
        "    up6 = LeakyReLU()(up6)\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge6)\n",
        "    conv6 = LeakyReLU()(conv6)\n",
        "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
        "    conv6 = LeakyReLU()(conv6)\n",
        "    up7 = Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv6))\n",
        "    up7 = LeakyReLU()(up7)\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge7)\n",
        "    conv7 = LeakyReLU()(conv7)\n",
        "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
        "    conv7 = LeakyReLU()(conv7)\n",
        "    up8 = Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LeakyReLU()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge8)\n",
        "    conv8 = LeakyReLU()(conv8)\n",
        "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
        "    conv8 = LeakyReLU()(conv8)\n",
        "\n",
        "    up9 = Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv8))\n",
        "    up9 = LeakyReLU()(up9)\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'tanh')(conv9)\n",
        "\n",
        "    model = Model(inputs,conv10)\n",
        "\n",
        "    model.compile(optimizer = 'adam', loss = tf.keras.losses.MeanSquaredError(), metrics = ['mae'])\n",
        "    #model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwRXg7yO1Yl4"
      },
      "source": [
        "These functions will be used for scaling the audio files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0B7nYrZp1YDt"
      },
      "outputs": [],
      "source": [
        "def scaled_in(matrix_spec):\n",
        "    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec + 46)/50\n",
        "    return matrix_spec\n",
        "def scaled_ou(matrix_spec):\n",
        "    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec -6 )/82\n",
        "    return matrix_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Training the Model, Let's see the shape of the data"
      ],
      "metadata": {
        "id": "hKxB6HspaCqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Lets reshape the input and the output data to 1-dimensional tensor for training\n",
        "\n",
        "- Converting an array to a one-dimensional tensor allows the model to more easily process the data.\n",
        "\n",
        "- A one-dimensional tensor is a simple linear array of data, which makes it easier for the model to understand the input and perform computations on it."
      ],
      "metadata": {
        "id": "vBSjdbQNatRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qaCMvQ-OoT99"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def training_unet(path_save_spectrogram, weights_path, epochs, batch_size):\n",
        "    \"\"\" This function will read noisy voice and clean voice spectrograms created by data_creation mode,\n",
        "    and train a Unet model on this dataset for epochs and batch_size specified. It saves best models to disk regularly.\n",
        "    \"\"\"\n",
        "    #load noisy voice & clean voice spectrograms created by data_creation mode\n",
        "    X_in = np.load(path_save_spectrogram +'noisy_voice_amp_db'+\".npy\")\n",
        "    X_ou = np.load(path_save_spectrogram +'voice_amp_db'+\".npy\")\n",
        "    #Model of noise to predict\n",
        "    X_ou = X_in - X_ou\n",
        "\n",
        "    #Check distribution\n",
        "    print(stats.describe(X_in.reshape(-1,1)))\n",
        "    print(stats.describe(X_ou.reshape(-1,1)))\n",
        "\n",
        "    #to scale between -1 and 1\n",
        "    X_in = scaled_in(X_in)\n",
        "    X_ou = scaled_ou(X_ou)\n",
        "\n",
        "    #Check shape of spectrograms\n",
        "    print(X_in.shape)\n",
        "    print(X_ou.shape)\n",
        "    #Check new distribution\n",
        "    print(stats.describe(X_in.reshape(-1,1)))\n",
        "    print(stats.describe(X_ou.reshape(-1,1)))\n",
        "\n",
        "\n",
        "    #Reshape for training\n",
        "    X_in = X_in[:,:,:]\n",
        "    X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
        "    X_ou = X_ou[:,:,:]\n",
        "    X_ou = X_ou.reshape(X_ou.shape[0],X_ou.shape[1],X_ou.shape[2],1)\n",
        "    # print(X_in.shape)\n",
        "    # print(X_out.shape)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
        "\n",
        "    generator_nn=unet()\n",
        "\n",
        "    #Save best models to disk during training\n",
        "    checkpoint = ModelCheckpoint(weights_path+'/model_unet_best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "    generator_nn.summary()\n",
        "\n",
        "    #Training\n",
        "    history = generator_nn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[checkpoint], verbose=1, validation_data=(X_test, y_test))\n",
        "    model_in_json = generator_nn.to_json()\n",
        "\n",
        "    #Saving Model\n",
        "    with open(weights_path+'model_unet.json','w') as json_file:\n",
        "      json_file.write(model_in_json)\n",
        "\n",
        "    #Plot training and validation loss (log scale)\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.plot(epochs, loss, label='Training loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation loss')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UXfC6QEH0cmI"
      },
      "outputs": [],
      "source": [
        "mkdir weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_X6lUAoWbix"
      },
      "source": [
        "## Model 2: Segmentation Model using ResNet101 Pre-trained model as Encoder.\n",
        "\n",
        "For we this we are using a Python Library with Neural Networks for Image Segmentation based on Keras and TensorFlow. Read more about it here [Segmentation Models](https://github.com/qubvel/segmentation_models#installation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QnCU29bQzUbm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def training_2(path_save_spectrogram, weights_path, epochs, batch_size):\n",
        "    \"\"\" This function will read noisy voice and clean voice spectrograms created by data_creation mode,\n",
        "    and train a Unet model on this dataset for epochs and batch_size specified. It saves best models to disk regularly\n",
        "    \"\"\"\n",
        "    #load noisy voice & clean voice spectrograms created by data_creation mode\n",
        "    X_in = np.load(path_save_spectrogram +'noisy_voice_amp_db'+\".npy\")\n",
        "    X_ou = np.load(path_save_spectrogram +'voice_amp_db'+\".npy\")\n",
        "    #Model of noise to predict\n",
        "    X_ou = X_in - X_ou\n",
        "\n",
        "    #Check distribution\n",
        "    print(stats.describe(X_in.reshape(-1,1)))\n",
        "    print(stats.describe(X_ou.reshape(-1,1)))\n",
        "\n",
        "    #to scale between -1 and 1\n",
        "    X_in = scaled_in(X_in)\n",
        "    X_ou = scaled_ou(X_ou)\n",
        "\n",
        "    #Check shape of spectrograms\n",
        "    print(X_in.shape)\n",
        "    print(X_ou.shape)\n",
        "    #Check new distribution\n",
        "    print(stats.describe(X_in.reshape(-1,1)))\n",
        "    print(stats.describe(X_ou.reshape(-1,1)))\n",
        "\n",
        "\n",
        "    #Reshape for training\n",
        "    X_in = X_in[:,:,:]\n",
        "    X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
        "    X_ou = X_ou[:,:,:]\n",
        "    X_ou = X_ou.reshape(X_ou.shape[0],X_ou.shape[1],X_ou.shape[2],1)\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
        "\n",
        "    import segmentation_models as sm\n",
        "    from segmentation_models import Unet\n",
        "    from keras.layers import Input, Conv2D\n",
        "    from keras.models import Model\n",
        "\n",
        "    # define number of channels\n",
        "    N = x_train.shape[-1]\n",
        "\n",
        "    base_model = Unet(backbone_name='resnet101', encoder_weights='imagenet')\n",
        "\n",
        "    inp = Input(shape=(None, None, N))\n",
        "    l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n",
        "    out = base_model(l1)\n",
        "\n",
        "    model = Model(inp, out, name=base_model.name)\n",
        "    BACKBONE = 'resnet101'\n",
        "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "    # preprocess input\n",
        "    x_train = preprocess_input(x_train)\n",
        "    x_val = preprocess_input(x_val)\n",
        "\n",
        "    # define model\n",
        "    model.compile(\n",
        "    'Adam',\n",
        "    loss = tf.keras.losses.MeanSquaredError(),\n",
        "     metrics = ['mae']\n",
        "    )\n",
        "\n",
        "    # fitting model\n",
        "    checkpoint = ModelCheckpoint(weights_path+'/model_ResNet.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[checkpoint]\n",
        "       )\n",
        "    #Saving model in Json file\n",
        "    model_in_json = model.to_json()\n",
        "    with open('model_ResNet.json','w') as json_file:\n",
        "      json_file.write(model_in_json)\n",
        "    #Plot training and validation loss (log scale)\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.plot(epochs, loss, label='Training loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation loss')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikR9hQs3D0ou"
      },
      "source": [
        "## Result of Experimentation\n",
        "\n",
        "*   **Observation 1:**  As you can see in the above 2 models both models results are very similar with 500 samples.  \n",
        "*   **Observation 2:** As the samples increase both models improves.\n",
        "*   **Observation 3:** Due to constraints in free version of colab we have to run seperate notebook if we want to run the notebook with samples more than 500.\n",
        "\n",
        "*   **Observation 4:** Below given are some of result of experimentation.\n",
        "\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1taQK-DD3CwCHYIq-DHq7x4_iOY6hdRYl\" height = 300 ></center>\n",
        "\n",
        "*   **Observation 5:** Other than U-net model we also experimented with latest and new models. Here are the top 3 results of experimentaion with 5000 samples.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1frb19Krm-GKgLBnURA3_YLTcfvJDsk8H\" height = 200 ></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEJcy9oF2VL"
      },
      "source": [
        "*   **Observation 6:** When we increase the number of samples to 10000\n",
        "we get our best models Unet model with ResNet 101 as encoder and Unet from scratch both show similar  loss which is  mae of **0.0402**.\n",
        "\n",
        "*  Now we will see how the Unet model performs when given a input.\n",
        "\n",
        "_________________________________________________________________\n",
        "# Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7qLX1IE4p7"
      },
      "source": [
        "### Process Diagram of creating data for Prediction\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1WcwX5xlTPVYDLqXutAgvtMyCEitFcybZ\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "heTUe7daI0K0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ea0ccb-a23e-492d-9c8e-81384dfcbe4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=103NEFLrqrs8qOFG08RS2bkuMHPlhKTZU\n",
            "To: /content/Best_weight_Unet.h5\n",
            "100% 23.6M/23.6M [00:01<00:00, 13.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--MnvC_MpsdU2giO8bhdWZA0HgPWR6se\n",
            "To: /content/Best_json_Unet.json\n",
            "100% 26.1k/26.1k [00:00<00:00, 41.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the best models .h5 and json file\n",
        "!gdown 103NEFLrqrs8qOFG08RS2bkuMHPlhKTZU\n",
        "!gdown 1--MnvC_MpsdU2giO8bhdWZA0HgPWR6se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8Hy0MMA1IE6A"
      },
      "outputs": [],
      "source": [
        "#Helper Functions\n",
        "def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):\n",
        "    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n",
        "\n",
        "    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n",
        "\n",
        "    # taking magnitude and phase of audio\n",
        "    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n",
        "    audio_reconstruct = librosa.core.istft(audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)\n",
        "\n",
        "    return audio_reconstruct\n",
        "\n",
        "\n",
        "def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft)  :\n",
        "    \"\"\"This functions reverts the matrix spectrograms to numpy audio\"\"\"\n",
        "\n",
        "    list_audio = []\n",
        "\n",
        "    nb_spec = m_mag_db.shape[0]\n",
        "\n",
        "    for i in range(nb_spec):\n",
        "\n",
        "        audio_reconstruct = magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, m_mag_db[i], m_phase[i])\n",
        "        list_audio.append(audio_reconstruct)\n",
        "\n",
        "    return np.vstack(list_audio)\n",
        "\n",
        "def inv_scaled_ou(matrix_spec):\n",
        "    \"inverse global scaling apply to noise models spectrograms\"\n",
        "    matrix_spec = matrix_spec * 82 + 6\n",
        "    return matrix_spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ovavIwHlZta5"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import soundfile as sf\n",
        "\n",
        "def prediction(weights_path, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
        "audio_output_prediction):\n",
        "    \"\"\" This function takes as input pretrained weights, noisy voice sound to denoise, predict\n",
        "    the denoise sound and save it to disk.\n",
        "    \"\"\"\n",
        "\n",
        "    # load json and create model\n",
        "    json_file = open('Best_json_Unet.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model/\n",
        "    loaded_model.load_weights('Best_weight_Unet.h5')\n",
        "    print(\"Loaded model from disk\")\n",
        "\n",
        "    # Extracting noise and voice from folder and convert to numpy\n",
        "    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n",
        "                                 frame_length, hop_length_frame, min_duration)\n",
        "\n",
        "    #Dimensions of squared spectrogram\n",
        "    dim_square_spec = int(n_fft / 2) + 1\n",
        "    print(dim_square_spec)\n",
        "\n",
        "    # Create Amplitude and phase of the sounds\n",
        "    m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
        "        audio, dim_square_spec, n_fft, hop_length_fft)\n",
        "\n",
        "    #global scaling to have distribution -1/1\n",
        "    X_in = scaled_in(m_amp_db_audio)\n",
        "    #Reshape for prediction\n",
        "    X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
        "    #Prediction using loaded network\n",
        "    X_pred = loaded_model.predict(X_in)\n",
        "    #Rescale back the noise model\n",
        "    inv_sca_X_pred = inv_scaled_ou(X_pred)\n",
        "    #Remove noise model from noisy speech\n",
        "    X_denoise = m_amp_db_audio - inv_sca_X_pred[:,:,:,0]\n",
        "    #Reconstruct audio from denoised spectrogram and phase\n",
        "    print(X_denoise.shape)\n",
        "    print(m_pha_audio.shape)\n",
        "    print(frame_length)\n",
        "    print(hop_length_fft)\n",
        "    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n",
        "    #Number of frames\n",
        "    nb_samples = audio_denoise_recons.shape[0]\n",
        "    #Save all frames in one file\n",
        "    denoise_long = audio_denoise_recons.reshape(1, nb_samples * frame_length)*10\n",
        "    # librosa.output.write_wav(dir_save_prediction + audio_output_prediction, denoise_long[0, :], 1000)\n",
        "    sf.write(dir_save_prediction + audio_output_prediction, denoise_long[0, :], 8000, 'PCM_24')\n",
        "    # wavfile.write(dir_save_prediction + audio_output_prediction, 1000, denoise_long[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AYmDOLGU3pr"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JQVhtR6nRIXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f38390-6092-4747-8245-e2f08dcd3520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from scipy.io.wavfile import write\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import math"
      ],
      "metadata": {
        "id": "hnUQIFdXMmn4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_energy(audio_file):\n",
        "    # Load the audio signal\n",
        "    y, sr = librosa.load(audio_file)\n",
        "\n",
        "    # Square each sample\n",
        "    squared_samples = y ** 2\n",
        "\n",
        "    # Calculate the energy (sum of squares)\n",
        "    energy = squared_samples.sum()\n",
        "\n",
        "    # Determine the duration of the signal\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "\n",
        "    # Alternatively, you can get the number of samples\n",
        "    num_samples = len(y)\n",
        "\n",
        "    # Calculate energy per duration\n",
        "    energy_per_duration = energy / duration\n",
        "\n",
        "    # Or calculate energy per number of samples\n",
        "    energy_per_sample = energy / num_samples\n",
        "\n",
        "    return energy_per_duration, energy_per_sample\n",
        "\n"
      ],
      "metadata": {
        "id": "QyTM4S6l2fHM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "Ec3Yw0Pq5m_M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "def packagewav(search_directory = '/content',target_directory = '/content/files_to_download',zip_filename = '/content/files_to_download.zip'):\n",
        "  # Step 2: Create the target directory if it does not exist\n",
        "  if not os.path.exists(target_directory):\n",
        "      os.makedirs(target_directory)\n",
        "\n",
        "  # Step 3: Find and move files that start with 'noise' or 'denoise'\n",
        "  for filename in os.listdir(search_directory):\n",
        "    if 'mixed' in filename or 'proc' in filename or 'denoised' in filename:\n",
        "        file_path = os.path.join(search_directory, filename)\n",
        "        if os.path.isfile(file_path):  # Ensure it's a file\n",
        "            shutil.move(file_path, target_directory)\n",
        "\n",
        "\n",
        "  # Step 4: Create a zip file containing the files in the target directory\n",
        "  shutil.make_archive(target_directory, 'zip', target_directory)\n",
        "\n",
        "  # Step 5: Download the zip file\n",
        "  files.download(zip_filename)"
      ],
      "metadata": {
        "id": "HLVmN42p1RTu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample code below USE THIS ONE\n",
        "**"
      ],
      "metadata": {
        "id": "cdBl8sqh9JK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "WWyNmIwi4rup"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yjiNr7pS5Fx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## THIS IS CORRECT CODE FOR ONE\n",
        "def produce_multifiles(folder_path):\n",
        "  audio2 = AudioSegment.from_file(\"audio_part_1.wav\")# babble noise\n",
        "  '''\n",
        "  folder_path: takes in a folder and process all the .wav files inside at db -> pass to neural net, each .wav file will have 10 random noises\n",
        "  db: adjust the noiseness of noise when combining\n",
        "  '''\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.wav'):\n",
        "        full_file_name = os.path.join(folder_path, file_name)\n",
        "        print(\"Processing:\")\n",
        "        print(file_name)\n",
        "        audio1_name = full_file_name\n",
        "        y, sr = librosa.load(audio1_name)\n",
        "        energy_per_duration2, energy_per_sample2 = calculate_energy(audio1_name)\n",
        "        ratio = math.sqrt(90.03581759295885/energy_per_duration2)\n",
        "        y = y*ratio\n",
        "        sf.write('newaudio.wav', y, sr)\n",
        "        audio1 = AudioSegment.from_file(\"newaudio.wav\")##clean file\n",
        "        random.seed(42)\n",
        "\n",
        "        for i in range(1,11):\n",
        "          # Add i dB to audio2\n",
        "          db = -11\n",
        "          audio2_new = audio2 + db #modify here\n",
        "          # Ensure the clip duration does not exceed the length of the shorter audio\n",
        "          clip_duration = min(len(audio1), len(audio2_new))\n",
        "          # Randomly select starting points for the clips\n",
        "          start_point_audio1 = random.randint(0, len(audio1) - clip_duration)\n",
        "          start_point_audio2 = random.randint(0, len(audio2_new) - clip_duration)\n",
        "\n",
        "          cropped_audio1 = audio1[start_point_audio1:start_point_audio1 + clip_duration]\n",
        "          cropped_audio2 = audio2_new[start_point_audio2:start_point_audio2 + clip_duration]\n",
        "\n",
        "          mixed_audio = cropped_audio1.overlay(cropped_audio2)\n",
        "          mixed_audio = mixed_audio.set_sample_width(2)\n",
        "\n",
        "          # Export the mixed audio\n",
        "          mixed_audio.export(f\"{file_name[:-4]}_mixed_{db}db_{i}.wav\", format=\"wav\")\n",
        "          y , sr = librosa.load(f\"{file_name[:-4]}_mixed_{db}db_{i}.wav\", sr=8000)\n",
        "\n",
        "          # Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "          write(f\"{file_name[:-4]}_mixed_{db}db_{i}.wav\", sr, y)\n",
        "          # files.download(f'mixed_audio8Khz+{i}.wav')\n",
        "          prediction('/content', '/content', '/content/', [f'{file_name[:-4]}_mixed_{db}db_{i}.wav'], f'{file_name[:-4]}_proc_{db}db_{i}.wav')\n",
        "          # files.download(f'denoise_mixed+{i}.wav')\n",
        "\n",
        "          input_file = f\"{file_name[:-4]}_mixed_{db}db_{i}.wav\"\n",
        "          output_file = f\"{file_name[:-4]}_mixed_{db}db_{i}.wav\"\n",
        "          audio = AudioSegment.from_file(input_file)\n",
        "          audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n",
        "\n",
        "          input_file = f\"{file_name[:-4]}_proc_{db}db_{i}.wav\"\n",
        "          output_file = f\"{file_name[:-4]}_proc_{db}db_{i}.wav\"\n",
        "          audio = AudioSegment.from_file(input_file)\n",
        "          audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n"
      ],
      "metadata": {
        "id": "KCmUlFhxMfjd",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR MANY DB LEVELS:"
      ],
      "metadata": {
        "id": "O_rUUMhMrNbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "GghK4Kl0PBnm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Average(lst):\n",
        "              return reduce(lambda a, b: a + b, lst) / len(lst)"
      ],
      "metadata": {
        "id": "ds4hLrCbPBNe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_energy_per_window(audio_file,window_duration=0.25,shift_duration = 0.125):\n",
        "    # Load the audio signal\n",
        "    y, sr = librosa.load(audio_file)\n",
        "\n",
        "    # Calculate the number of samples for the window and shift\n",
        "    window_size = int(window_duration * sr)\n",
        "    shift_size = int(shift_duration * sr)\n",
        "\n",
        "    # List to store all noise per sample values\n",
        "    noise_per_sample_list = []\n",
        "\n",
        "    # Iterate over the signal in window_size chunks with shift_size steps\n",
        "    for start in range(0, len(y) - window_size + 1, shift_size):\n",
        "        window = y[start:start + window_size]\n",
        "        squared_samples = window ** 2\n",
        "        energy = squared_samples.sum()\n",
        "        num_samples = len(window)\n",
        "\n",
        "        # Calculate energy per sample\n",
        "        noise_per_sample = energy / num_samples\n",
        "\n",
        "        # Append the noise per sample value to the list\n",
        "        noise_per_sample_list.append(noise_per_sample)\n",
        "\n",
        "    # Sort the list in descending order and get the top 10 values\n",
        "    top_10_noise_per_sample = sorted(noise_per_sample_list, reverse=True)[:10]\n",
        "    bottom_10_noise_per_sample = sorted(noise_per_sample_list, reverse=False)[:10]\n",
        "    return top_10_noise_per_sample,bottom_10_noise_per_sample\n"
      ],
      "metadata": {
        "id": "SZibYe6tPPEm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## AUTOMATED FOR MANY DB LEVELS\n",
        "def automate_multifiles(folder_path,attenuation):\n",
        "  audio2 = AudioSegment.from_file(\"pharrell-williams-happy-video_audio_low.wav\")# babble noise\n",
        "  '''\n",
        "  folder_path: takes in a folder and process all the .wav files inside at db -> pass to neural net, each .wav file will have 10 random noises\n",
        "  db: adjust the noiseness of noise when combining\n",
        "  '''\n",
        "  automate_noise_dict = {}\n",
        "  ratio_list=[]\n",
        "  energy_list=[]\n",
        "  bottom_energy_list=[]\n",
        "  db_list=[]\n",
        "  window_energy_list=[]\n",
        "  ratio_top_bottom_list=[]\n",
        "\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.wav'):\n",
        "        full_file_name = os.path.join(folder_path, file_name)\n",
        "        print(\"Processing:\")\n",
        "        print(file_name)\n",
        "        audio1_name = full_file_name\n",
        "\n",
        "        audio1 = AudioSegment.from_file(audio1_name)##clean file\n",
        "        one_sec = AudioSegment.silent(duration=1000)\n",
        "        audio1 = audio1 + one_sec\n",
        "        # for different results seed needs to be off\n",
        "        # random.seed(42)\n",
        "\n",
        "\n",
        "        # i represents db levels, iterating through\n",
        "        for i in range(-13,-14,-2):\n",
        "          db =i\n",
        "          # j represents noise levels\n",
        "          for j in range(1,6):\n",
        "            audio2_new = audio2 + db\n",
        "            clip_duration = min(len(audio1), len(audio2_new))\n",
        "            # print(f\"lenght of clean audio {len(audio1)}, length of music {len(audio2_new)}\")\n",
        "            # Randomly select starting points for the clips\n",
        "            start_point_audio1 = random.randint(0, len(audio1) - clip_duration)\n",
        "            start_point_audio2 = random.randint(0, len(audio2_new) - clip_duration)\n",
        "\n",
        "            cropped_audio1 = audio1[start_point_audio1:start_point_audio1 + clip_duration]\n",
        "            cropped_audio2 = audio2_new[start_point_audio2:start_point_audio2 + clip_duration]\n",
        "            mixed_audio = cropped_audio1.overlay(cropped_audio2)\n",
        "            mixed_audio = mixed_audio.set_sample_width(2)\n",
        "\n",
        "\n",
        "            mixed_audio.export(f\"{file_name[:-4]}_mixed_16k_original_{i}db_{j}.wav\", format=\"wav\", parameters=[\"-ar\", \"16000\"])#original mixed audio @ 16000Hz\n",
        "\n",
        "            #calculating SNR\n",
        "            # _,energy = calculate_energy(f\"{file_name[:-4]}_mixed_16k_original_{i}db_{j}.wav\")\n",
        "            # energy_list.append(energy)\n",
        "            # top_ten_energy_per_window, bottom_ten_enerygy_per_window = calculate_energy_per_window(f\"{file_name[:-4]}_mixed_16k_original_{i}db_{j}.wav\")\n",
        "\n",
        "            # avg_top_ten = Average(top_ten_energy_per_window)\n",
        "            # avg_bottom_ten = Average(bottom_ten_enerygy_per_window)\n",
        "            # ratio_top_to_bottom = avg_top_ten/avg_bottom_ten\n",
        "\n",
        "\n",
        "            # print(f\"Voice to noise ratio of noisy file at {db}db is {ratio_top_to_bottom}\")\n",
        "\n",
        "            y , sr = librosa.load(f\"{file_name[:-4]}_mixed_16k_original_{i}db_{j}.wav\", sr=8000) #load the original mixed audio @ 8000Hz\n",
        "\n",
        "            # Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "            write(f\"{file_name[:-4]}_mixed_8k_{i}db_{j}.wav\", sr, y)\n",
        "\n",
        "            #check the signal to noise ratio if needed\n",
        "            # if ratio_top_to_bottom < 100:\n",
        "            #   prediction('/content', '/content', '/content/', [f'{file_name[:-4]}_mixed_8k_{i}db_{j}.wav'], f'{file_name[:-4]}_proc_{i}db_{j}.wav')\n",
        "            # else:\n",
        "            #   input_file = f\"{file_name[:-4]}_mixed_8k_{i}db_{j}.wav\"\n",
        "            #   output_file = f\"{file_name[:-4]}_proc_{i}db_{j}.wav\"\n",
        "            #   audio = AudioSegment.from_file(input_file)\n",
        "            #   audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n",
        "\n",
        "              #upsample everything to 16k\n",
        "            prediction('/content', '/content', '/content/', [f'{file_name[:-4]}_mixed_8k_{i}db_{j}.wav'], f'{file_name[:-4]}_proc_{i}db_{j}.wav')\n",
        "            audio = AudioSegment.from_file(f\"{file_name[:-4]}_proc_{i}db_{j}.wav\")\n",
        "            audio.export(f\"{file_name[:-4]}_denoised_{i}db_{j}.wav\", format=\"wav\",parameters=[\"-ar\", \"16000\",\"-sample_fmt\", \"s16\"])\n",
        "\n",
        "\n",
        "            #residual connection\n",
        "            originalaudio = AudioSegment.from_file(f\"{file_name[:-4]}_mixed_16k_original_{i}db_{j}.wav\")\n",
        "            originalaudio = originalaudio - attenuation\n",
        "\n",
        "            denoisedaudio = AudioSegment.from_file(f\"{file_name[:-4]}_denoised_{i}db_{j}.wav\")\n",
        "            combined = originalaudio.overlay(denoisedaudio)\n",
        "            combined.export(f\"{file_name[:-4]}_proc_{i}db_{j}.wav\", format=\"wav\",parameters=[\"-ar\", \"16000\",\"-sample_fmt\", \"s16\"])\n",
        "\n",
        "            os.remove(f\"{file_name[:-4]}_denoised_{i}db_{j}.wav\")\n",
        "            os.remove(f\"{file_name[:-4]}_mixed_8k_{i}db_{j}.wav\")\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "8jBIMUy8rQMJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automate_noise_dict = automate_multifiles('/content/wav_files_toprocess',attenuation=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4veaxousjR9",
        "outputId": "984135d7-eee7-4eed-ef4b-291140b0f5bc",
        "collapsed": true
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing:\n",
            "04-long_speech_male.wav\n",
            "Loaded model from disk\n",
            "128\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "(13, 128, 128)\n",
            "(13, 128, 128)\n",
            "8064\n",
            "63\n",
            "Loaded model from disk\n",
            "128\n",
            "1/1 [==============================] - 1s 823ms/step\n",
            "(13, 128, 128)\n",
            "(13, 128, 128)\n",
            "8064\n",
            "63\n",
            "Loaded model from disk\n",
            "128\n",
            "1/1 [==============================] - 1s 788ms/step\n",
            "(13, 128, 128)\n",
            "(13, 128, 128)\n",
            "8064\n",
            "63\n",
            "Loaded model from disk\n",
            "128\n",
            "1/1 [==============================] - 1s 786ms/step\n",
            "(13, 128, 128)\n",
            "(13, 128, 128)\n",
            "8064\n",
            "63\n",
            "Loaded model from disk\n",
            "128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff78b55d120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 776ms/step\n",
            "(13, 128, 128)\n",
            "(13, 128, 128)\n",
            "8064\n",
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "packagewav()"
      ],
      "metadata": {
        "id": "zGcsBpvxspbq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c331cdde-9dd2-497b-e633-08e82c12653e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a4262148-331b-4bbb-ae19-8b0ee5b910ea\", \"files_to_download.zip\", 7613366)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree('/content/files_to_download')  # Delete the directory\n",
        "os.remove('/content/files_to_download.zip')  # Delete the zip file"
      ],
      "metadata": {
        "id": "sWxqUFVBsrZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## CHECK AUTOMATE CONSISTENT WITH PRODUCE FILES\n",
        "def check_automate(folder_path, automate_noise_dict):\n",
        "  audio2 = AudioSegment.from_file(\"audio_part_1.wav\")# babble noise\n",
        "  '''\n",
        "  folder_path: takes in a folder and process all the .wav files inside at db -> pass to neural net, each .wav file will have 10 random noises\n",
        "  db: adjust the noiseness of noise when combining\n",
        "  '''\n",
        "  print(automate_noise_dict)\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.wav'):\n",
        "        full_file_name = os.path.join(folder_path, file_name)\n",
        "        print(\"Processing:\")\n",
        "        print(file_name)\n",
        "        audio1_name = full_file_name\n",
        "        y, sr = librosa.load(audio1_name)\n",
        "        energy_per_duration2, energy_per_sample2 = calculate_energy(audio1_name)\n",
        "        ratio = math.sqrt(90.03581759295885/energy_per_duration2)\n",
        "        y = y*ratio\n",
        "        sf.write('newaudio.wav', y, sr)\n",
        "        audio1 = AudioSegment.from_file(\"newaudio.wav\")##clean file\n",
        "        # for variation seed needs to be off\n",
        "        #random.seed(42)\n",
        "\n",
        "        for i in range(1,11):\n",
        "          # range_start is important for indexing properly\n",
        "          range_start = 1\n",
        "\n",
        "          # set db level\n",
        "          db = -11\n",
        "          audio2_new = audio2 + db\n",
        "\n",
        "          # Ensure the clip duration does not exceed the length of the shorter audio\n",
        "          clip_duration = min(len(audio1), len(audio2_new))\n",
        "          # Randomly select starting points for the clips\n",
        "          #start_point_audio1 = random.randint(0, len(audio1) - clip_duration)\n",
        "          #start_point_audio2 = random.randint(0, len(audio2_new) - clip_duration)\n",
        "          #automate_noise_dict[file_name][db]\n",
        "          start_point_audio1 = automate_noise_dict[file_name][db][i-range_start][0]\n",
        "          start_point_audio2 = automate_noise_dict[file_name][db][i-range_start][1]\n",
        "          #print(f'db: {db}, i{i}: {automate_noise_dict[file_name][db][i-range_start]}')\n",
        "\n",
        "\n",
        "          cropped_audio1 = audio1[start_point_audio1:start_point_audio1 + clip_duration]\n",
        "          cropped_audio2 = audio2_new[start_point_audio2:start_point_audio2 + clip_duration]\n",
        "\n",
        "          mixed_audio = cropped_audio1.overlay(cropped_audio2)\n",
        "          mixed_audio = mixed_audio.set_sample_width(2)\n",
        "\n",
        "          # Export the mixed audio\n",
        "          mixed_audio.export(f\"{file_name[:-4]}_mixed_{db}db_{i}_check.wav\", format=\"wav\")\n",
        "          # y , sr = librosa.load(f\"{file_name[:-4]}_mixed_{db}db_{i}_check.wav\", sr=8000)\n",
        "\n",
        "          # # Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "          # write(f\"{file_name[:-4]}_mixed_{db}db_{i}_check.wav\", sr, y)\n",
        "          # # files.download(f'mixed_audio8Khz+{i}.wav')\n",
        "          # prediction('/content', '/content', '/content/', [f'{file_name[:-4]}_mixed_{db}db_{i}_check.wav'], f'{file_name[:-4]}_proc_{db}db_{i}_check.wav')\n",
        "          # # files.download(f'denoise_mixed+{i}.wav')\n",
        "\n",
        "          # input_file = f\"{file_name[:-4]}_mixed_{db}db_{i}_check.wav\"\n",
        "          # output_file = f\"{file_name[:-4]}_mixed_{db}db_{i}_check.wav\"\n",
        "          # audio = AudioSegment.from_file(input_file)\n",
        "          # audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n",
        "\n",
        "          # input_file = f\"{file_name[:-4]}_proc_{db}db_{i}_check.wav\"\n",
        "          # output_file = f\"{file_name[:-4]}_proc_{db}db_{i}_check.wav\"\n",
        "          # audio = AudioSegment.from_file(input_file)\n",
        "          # audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])"
      ],
      "metadata": {
        "id": "s0KMz5CrtFu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_automate('/content/wav_files_toprocess', automate_noise_dict)"
      ],
      "metadata": {
        "id": "MbtHnSiN7l7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "packagewav()"
      ],
      "metadata": {
        "id": "DCOtF2I27t4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree('/content/files_to_download')  # Delete the directory\n",
        "os.remove('/content/files_to_download.zip')  # Delete the zip file"
      ],
      "metadata": {
        "id": "yXBCiCtU7wJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import random\n",
        "import librosa\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "def repeat_to_match_length(sound, length):\n",
        "    \"\"\"Repeat an AudioSegment to match the specified length.\"\"\"\n",
        "    repeated_sound = sound\n",
        "    while len(repeated_sound) < length:\n",
        "        repeated_sound += sound\n",
        "    return repeated_sound[:length]\n",
        "\n",
        "audio1 = AudioSegment.from_file(\"The trains from Boston 2.m4a\")  # Clean file\n",
        "audio2 = AudioSegment.from_file(\"dogbark.wav\")  # Babble noise\n",
        "\n",
        "for i in range(-20, -6, 2):\n",
        "    # Adjust the volume of audio2\n",
        "    audio2_new = audio2 + i\n",
        "\n",
        "    # Ensure audio2_new is the same length as audio1\n",
        "    audio2_new = repeat_to_match_length(audio2_new, len(audio1))\n",
        "\n",
        "    # Overlay the audio files\n",
        "    mixed_audio = audio1.overlay(audio2_new)\n",
        "\n",
        "    # Set sample width to 2 bytes\n",
        "    mixed_audio = mixed_audio.set_sample_width(2)\n",
        "\n",
        "    # Export the mixed audio\n",
        "    mixed_audio.export(f\"mixed+{i}.wav\", format=\"wav\")\n",
        "\n",
        "    # Load the audio file using librosa\n",
        "    y, sr = librosa.load(f\"mixed+{i}.wav\", sr=8000)\n",
        "\n",
        "    # Save the file in 8000 Hz sample rate\n",
        "    write(f\"mixed+{i}.wav\", sr, y)\n",
        "\n",
        "    # Assuming `prediction` is a defined function for denoising\n",
        "    prediction('/content', '/content', '/content/', [f'mixed+{i}.wav'], f'denoise+{i}.wav')\n",
        "\n",
        "    # Convert and download mixed audio file\n",
        "    input_file = f\"mixed+{i}.wav\"\n",
        "    output_file = f\"mixed+{i}.wav\"\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n",
        "    files.download(f\"mixed+{i}.wav\")\n",
        "\n",
        "    # Convert and download denoised audio file\n",
        "    input_file = f\"denoise+{i}.wav\"\n",
        "    output_file = f\"denoise+{i}.wav\"\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    audio.export(output_file, format=\"wav\", parameters=[\"-acodec\", \"pcm_s16le\"])\n",
        "    files.download(f'denoise+{i}.wav')\n",
        "\n",
        "    print(f\"Processed and exported files: mixed+{i}.wav and denoise+{i}.wav\")\n"
      ],
      "metadata": {
        "id": "HVaZVHL8TvST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from pydub import AudioSegment\n",
        "import random"
      ],
      "metadata": {
        "id": "EXZh_nH4sH_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cuYWjILjc1xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_with_babble(audio_file, babble_noise, output_file, starting_sr, noiselvl=-9):\n",
        "  '''\n",
        "    combine an audio file with babble noise, there should be 3seconds of noise before and after the audio file.\n",
        "    noiselvl = the noise level adjustment of noise\n",
        "    starting_sr = -1 means random, or range from 1,000,000 to 11,000,000\n",
        "\n",
        "  '''\n",
        "  # Load the audio file and babble noise\n",
        "\n",
        "  audio = AudioSegment.from_wav(audio_file)\n",
        "  noise = AudioSegment.from_wav(babble_noise)\n",
        "\n",
        "  # Ensure both audio and noise are at 8kHz\n",
        "  audio = audio.set_frame_rate(8000)\n",
        "  noise = noise.set_frame_rate(8000)\n",
        "  noise = noise + noiselvl\n",
        "  start = 1_000_000\n",
        "  end = 11_000_000\n",
        "\n",
        "  blank_frames = 3000\n",
        "  noise_duration = len(audio) + 2 * blank_frames\n",
        "\n",
        "  # Create the babble noise segment of the required duration\n",
        "  if len(noise) > noise_duration:\n",
        "      if starting_sr >= start and starting_sr<=end:\n",
        "        start_point = starting_sr/8000*1000\n",
        "        noise_segment = noise[start_point:start_point + noise_duration]\n",
        "      elif starting_sr == -1:\n",
        "        start_point = random.randint(start/8000*1000, end/8000*1000)\n",
        "        noise_segment = noise[start_point:start_point + noise_duration]\n",
        "      else:\n",
        "        print('Starting_sampling_rate error!')\n",
        "  else:\n",
        "      # If the noise is shorter than the required duration, loop the noise\n",
        "      noise_segment = noise * (noise_duration // len(noise) + 1)\n",
        "      noise_segment = noise_segment[:noise_duration]\n",
        "\n",
        "  # Insert the audio file 3 seconds into the babble noise\n",
        "  combined = noise_segment.overlay(audio, position=blank_frames)\n",
        "\n",
        "  # Export the combined audio\n",
        "  combined.export(output_file, format=\"wav\")\n",
        "  print(f\"Saved combined audio to {output_file}, original audio length = {len(audio) / 1000} seconds\")\n",
        "\n",
        "def process_audio_files(root_folder, babble_noise_file):\n",
        "    # Loop over all subfolders in the root folder\n",
        "    for subdir, _, _ in os.walk(root_folder):\n",
        "        # Find all .wav files in the current subfolder\n",
        "        wav_files = glob.glob(os.path.join(subdir, '*.wav'))\n",
        "\n",
        "        for wav_file in wav_files:\n",
        "            # Create the output filename\n",
        "            output_file = os.path.join(subdir, f\"noisy_{os.path.basename(wav_file)}\")\n",
        "            # Combine the wav file with the babble noise\n",
        "            combine_with_babble(wav_file, babble_noise_file, output_file,42)"
      ],
      "metadata": {
        "id": "fzodVFr1tbE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_with_babble('3853-163249-0000.wav','audio_part_1.wav','combined.wav',starting_sr=-1)"
      ],
      "metadata": {
        "id": "rEKzDCIfZCok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('combined.wav')"
      ],
      "metadata": {
        "id": "gS03kXsSZwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"organized_audio_files\"\n",
        "babble_noise_file = \"path_to_your_babble_noise_file.wav\"  # Update with the correct path to the babble noise file\n",
        "\n",
        "# Process the audio files\n",
        "process_audio_files(root_folder, babble_noise_file)"
      ],
      "metadata": {
        "id": "2QzEo9_gt7CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('mixed_audio.wav')"
      ],
      "metadata": {
        "id": "hXgLJx9-MpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y , sr = librosa.load(\"mixed_audio.wav\", sr=8000)\n",
        "\n",
        "\n",
        "# Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "write(\"mixed_audio8Khz.wav\", sr, y)\n",
        "files.download('mixed_audio8Khz.wav')"
      ],
      "metadata": {
        "id": "6n25stppCBYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('mixed_audio8Khz.wav')"
      ],
      "metadata": {
        "id": "4WMz1GqEHqNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['mixed_audio8Khz.wav'],\n",
        "        'denoise_mixed.wav')\n",
        "files.download('denoise_mixed.wav')"
      ],
      "metadata": {
        "id": "kbGJDi1VC-Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['dog_noise.wav'],\n",
        "        'denoise_dog.wav')"
      ],
      "metadata": {
        "id": "JjLIk5d_IMls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('denoise_dog.wav')"
      ],
      "metadata": {
        "id": "FJAun0vaITJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "audio1 = AudioSegment.from_file(\"Original_clean.m4a\")\n",
        "audio2 = AudioSegment.from_file(\"crowd-talking2.mp3\")\n",
        "\n",
        "# Combine the audio files\n",
        "overlap_duration = 6000  # adjust as needed\n",
        "audio2 = audio2 + 15\n",
        "\n",
        "# Fade in the second audio\n",
        "audio2 = audio2.fade_in(overlap_duration)\n",
        "\n",
        "max_start_point = len(audio2) - overlap_duration\n",
        "start_point = random.randint(0, max_start_point)\n",
        "\n",
        "# Extract a 6-second segment from audio2 starting from the random point\n",
        "audio2_segment = audio2[start_point:start_point + overlap_duration]\n",
        "\n",
        "# Fade in the selected segment from audio2\n",
        "audio2_segment = audio2_segment.fade_in(overlap_duration)\n",
        "\n",
        "# Calculate the starting point for audio2_segment to overlap with audio1\n",
        "overlap_start = len(audio1) - overlap_duration\n",
        "\n",
        "# Mix the audio files by overlaying audio2_segment onto audio1\n",
        "mixed_audio = audio1.overlay(audio2_segment, position=overlap_start)\n",
        "\n",
        "\n",
        "\n",
        "# Export the mixed audio\n",
        "mixed_audio.export(\"babble_noise.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "_VCE0VYbJ1FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('babble_noise.wav')"
      ],
      "metadata": {
        "id": "5UCrXHzRKGgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y , sr = librosa.load(\"babble_noise.wav\", sr=8000)"
      ],
      "metadata": {
        "id": "nyEadYZsbJ5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "ipd.Audio(y, rate=sr)"
      ],
      "metadata": {
        "id": "L1F0MX2ubuxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.wavfile import write\n",
        "\n",
        "# Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "write(\"babble_noise8Khz.wav\", sr, y)"
      ],
      "metadata": {
        "id": "k8d3KvfScFy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('babble_noise8Khz.wav')"
      ],
      "metadata": {
        "id": "1iXBQigBcf60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('babble_noise.wav')"
      ],
      "metadata": {
        "id": "VbKJfLuLbdP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, sr = librosa.load(\"babble_noise8Khz.wav\", sr=8000)\n",
        "\n",
        "# Find the length of the audio in samples\n",
        "total_samples = len(y)\n",
        "\n",
        "# Find the midpoint\n",
        "midpoint = total_samples // 2\n",
        "\n",
        "# Clip half of the audio\n",
        "clipped_audio = y[:midpoint]"
      ],
      "metadata": {
        "id": "x5HzlydnfdmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.wavfile import write\n",
        "\n",
        "# Assuming you already loaded the audio and assigned it to y and sr as you mentioned\n",
        "write(\"babble_noise8Khz_half.wav\", sr, clipped_audio)"
      ],
      "metadata": {
        "id": "2cGgm31-fh9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "from google.colab import files\n",
        "Audio('babble_noise8Khz_half.wav')\n",
        "files.download('babble_noise8Khz_half.wav')"
      ],
      "metadata": {
        "id": "4SV2z3AXflht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['babble_noise8Khz_half.wav'],\n",
        "        'denoise_babble.wav')"
      ],
      "metadata": {
        "id": "gCx2iMQyKUmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('denoise_babble.wav')\n",
        "files.download('denoise_babble.wav')"
      ],
      "metadata": {
        "id": "HdHTBNfxffIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['Motorcycle K OFF.m4a'],\n",
        "        'denoise_motorcycle.wav')"
      ],
      "metadata": {
        "id": "PWITSpWLfj-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('denoise_motorcycle.wav')"
      ],
      "metadata": {
        "id": "WXirOgsGfq6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['Chip eatin K OFF.m4a'],\n",
        "        'denoise_chip.wav')"
      ],
      "metadata": {
        "id": "c6GzYN5dUvOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['Dogs barking K OFF.m4a'],\n",
        "        'denoise_dog.wav')"
      ],
      "metadata": {
        "id": "KuQP_Z0BU7z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['Hip-hop K OFF.m4a'],\n",
        "        'denoise_Hip-hop.wav')"
      ],
      "metadata": {
        "id": "rz9VPbQzVF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content', '/content', '/content/', ['Whitney Pop K OFF.m4a'],\n",
        "        'denoise_whitney.wav')"
      ],
      "metadata": {
        "id": "t24fYYUgVV70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbl_eMAmr0pp"
      },
      "source": [
        "## Prediction on the street audio with different background noises\n",
        "\n",
        "https://github.com/alexander-prutko/mil-audio-denoising/tree/main/audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRUooysvsHS8"
      },
      "outputs": [],
      "source": [
        "!gdown 1xq4Yl_BvkCYXKKKXd7P5JwW9GPiLyK5r\n",
        "!gdown 1XOzV_TyLkFl79QQM5MPrPZg1--cn0lA9\n",
        "!gdown 16zI4lqsGuGgrAiOUQR-Caau_Qn8gvxPY\n",
        "!gdown 1rvacwhsnOknQclhfTqbmj5NwuKL7qDtg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbFVNCbEsn-Q"
      },
      "outputs": [],
      "source": [
        "#Playing the audio_ex_01_80.wav\n",
        "from IPython.display import Audio\n",
        "Audio('/content/audio_ex_01_80.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ebo277otk3d"
      },
      "outputs": [],
      "source": [
        "prediction('/content', '/content', '/content/', ['audio_ex_01_80.wav'],\n",
        "        'denoise_ex_01_80.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYLZEFrNvXny"
      },
      "outputs": [],
      "source": [
        "#Playing the Denoised audio\n",
        "from IPython.display import Audio\n",
        "Audio('denoise_ex_01_80.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLhxxXNv44zv"
      },
      "source": [
        "- The output audio was denoised but the audio was not completed fully.\n",
        "- So the input audio should be above 5 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9W2m5i9451_"
      },
      "source": [
        "## Now let's try with a song to see, how our model performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFmWfGornBti"
      },
      "outputs": [],
      "source": [
        "!gdown 1EKE_uy5ZKBOSYJ72ZjRsxhUmAuQLDcvx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clXNEusu7xpg"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "Audio('I like me.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR7trLlH798w"
      },
      "outputs": [],
      "source": [
        "prediction('/content', '/content', '/content/', ['I like me.wav'],\n",
        "        'denoised_ilikeme.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Bly3yTCSVu"
      },
      "source": [
        "Prediction on music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzgd35vD9crL"
      },
      "outputs": [],
      "source": [
        "#Playing the Denoised audio\n",
        "from IPython.display import Audio\n",
        "Audio('denoised_ilikeme.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm4Se51jCTN-"
      },
      "source": [
        "## Observations:\n",
        "- The model is able to denoise the audio when it has single noise at a time.\n",
        "- If the input of the audio is less than 5 sec, the prediction was not complete.\n",
        "- The song had different musics playing at a time, but the prediction was quite good\n",
        "- We need more audio samples with multiple noise played at time, to improve the performance of the model further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kJp-oXV-uCY"
      },
      "source": [
        "Let's use the latest model **DeeplabsV3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgI-EMnx_Ek6"
      },
      "outputs": [],
      "source": [
        "!gdown 1OTq7eT2HZoyTsWjJ3SCl8dnxRu0aj2-y\n",
        "!gdown 1AC1U5jDl3qNAQpfseRoja9XdzfZd4Z5z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJNK7b5X-3eH"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import soundfile as sf\n",
        "\n",
        "def prediction_deeplab(weights_path, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
        "audio_output_prediction):\n",
        "    \"\"\" This function takes as input pretrained weights, noisy voice sound to denoise, predict\n",
        "    the denoise sound and save it to disk.\n",
        "    \"\"\"\n",
        "\n",
        "    # load json and create model\n",
        "    json_file = open('/content/drive/MyDrive/model_depplab.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model/\n",
        "    loaded_model.load_weights('/content/drive/MyDrive/model_deep_encoded.h5')\n",
        "    print(\"Loaded model from disk\")\n",
        "\n",
        "    # Extracting noise and voice from folder and convert to numpy\n",
        "    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n",
        "                                 frame_length, hop_length_frame, min_duration)\n",
        "\n",
        "    #Dimensions of squared spectrogram\n",
        "    dim_square_spec = int(n_fft / 2) + 1\n",
        "    print(dim_square_spec)\n",
        "\n",
        "    # Create Amplitude and phase of the sounds\n",
        "    m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
        "        audio, dim_square_spec, n_fft, hop_length_fft)\n",
        "\n",
        "    #global scaling to have distribution -1/1\n",
        "    X_in = scaled_in(m_amp_db_audio)\n",
        "    X_in = np.dstack([X_in] * 3)\n",
        "    #Reshape for prediction\n",
        "    X_in = X_in.reshape(X_in.shape[0],128,128,3)\n",
        "    #Prediction using loaded network\n",
        "    X_pred = loaded_model.predict(X_in)\n",
        "    #Rescale back the noise model\n",
        "    inv_sca_X_pred = inv_scaled_ou(X_pred)\n",
        "    #Remove noise model from noisy speech\n",
        "    X_denoise = m_amp_db_audio - inv_sca_X_pred[:,:,:,0]\n",
        "    #Reconstruct audio from denoised spectrogram and phase\n",
        "    print(X_denoise.shape)\n",
        "    print(m_pha_audio.shape)\n",
        "    print(frame_length)\n",
        "    print(hop_length_fft)\n",
        "    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n",
        "    #Number of frames\n",
        "    nb_samples = audio_denoise_recons.shape[0]\n",
        "    #Save all frames in one file\n",
        "    denoise_long = audio_denoise_recons.reshape(1, nb_samples * frame_length)*10\n",
        "    # librosa.output.write_wav(dir_save_prediction + audio_output_prediction, denoise_long[0, :], 1000)\n",
        "    sf.write(dir_save_prediction + audio_output_prediction, denoise_long[0, :], 8000, 'PCM_24')\n",
        "    # wavfile.write(dir_save_prediction + audio_output_prediction, 1000, denoise_long[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXzinn1FB3pR"
      },
      "outputs": [],
      "source": [
        "prediction_deeplab('/content', '/content', '/content/', ['Final_Output_Noisy_Audio_Sample_Roshan (1).wav'],\n",
        "        'denoise_audio_deeplab.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpO7ZI_BP5BP"
      },
      "outputs": [],
      "source": [
        "d#Playing the Denoised audio\n",
        "from IPython.display import Audio\n",
        "Audio('denoise_audio_deeplab.wav')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGw9JnxbNHl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRee1h1TPAGp"
      },
      "source": [
        "________________________________________________\n",
        "### Conclusion\n",
        "\n",
        "*   A deep learing audio denoiser is presented in the given project which denoises envionmental noises.\n",
        "*   By using a magnitude spectrogram representation of sound, the audio denoising problem has been transformed into an image processing problem, simplifying its resolution.\n",
        "\n",
        "*   The Noise to remove has been modelled by a U-Net and Resnet 101 encoder, out of which the Unet model performed better.\n",
        "\n",
        "*  The predictions are satisfactory and upon increasing audio amplitude the voice become more clear.\n",
        "\n",
        "________________________________________________\n",
        "\n",
        "# Deployment\n",
        "\n",
        "- The Model has been deployed in **streamlit**.\n",
        "- Streamlit is an open source app framework in Python language.\n",
        "- It helps us create web apps for data science and machine learning in a short time.[Click to know more](https://docs.streamlit.io/)\n",
        "\n",
        "- App link: [here](https://aravindsriraj-audio-denoising-app-4yzf1z.streamlit.app/)\n",
        "\n",
        "- Demo: https://drive.google.com/file/d/1hIOLWJ99UMjMrlMdj-fV_ihbv5kFhE99/view?usp=share_link\n",
        "\n",
        "\n",
        "______________________________________________________________\n",
        "\n",
        "# Future Scope\n",
        "\n",
        "*   By training a machine learning model on a large dataset of noisy and clean audio signals, it may be possible to create denoising algorithms that are more effective at removing specific types of noise.\n",
        "*    Another area for development is the integration of audio denoising tools into other systems and applications, such as audio recording and editing software or voice recognition systems.\n",
        "*   Overall, there is a lot of potential for further development in this field, and continued research and development is likely to result in more advanced and effective denoising algorithms that can be used in a variety of applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvCsKhi2gliN"
      },
      "source": [
        "_____________________________________________________________\n",
        "# References\n",
        "\n",
        "- https://arxiv.org/pdf/1703.08019.pdf\n",
        "- https://arxiv.org/pdf/1811.11307.pdf\n",
        "- https://github.com/qubvel/segmentation_models\n",
        "- https://segmentation-models.readthedocs.io/en/latest/tutorial.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RRee1h1TPAGp"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}